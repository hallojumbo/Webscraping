from urllib.parse import urlparse
from bs4 import BeautifulSoup
import requests
import hunspell #cyhunspell
import whois #python-whois
import datetime
import re
from threading import Thread, Lock
from queue import Queue

# de parameters die wij gaan toevoegen aan het neural network zijn
# 1: Database voor scam triggerwoorden https://instantly.ai/blog/spam-trigger-words
def check_keywords(text_lowered, file):
    f = open(file, "r")
    count = 0
    for word in f.readlines():
        count += text_lowered.count(word.replace("\n", ""))
    return count

# 2: De hoeveelheid foutgespelde woorden
def spell_check(text_formatted):
    h = hunspell.Hunspell()
    misspelled = []
    for word in text_formatted:
        if not h.spell(word) and len(h.suggest(word)) > 3 and misspelled.count(word) == 0:
            misspelled.append(word)
    return len(misspelled)

# 3: https of https
# 4: lengte van de url
def urlcheck(url):
    if url[:5] == "https":
        return ["https", len(urlparse(url).netloc)]
    return ["http", len(urlparse(url).netloc)]

# 5: als de method post wordt gebruikt
def check_POST(page):
    return page.lower().count("post") * 4 / len(page) * 100

# 6: bij image preloading
def check_preloads(page):
    return page.lower().count("preload") * 7 / len(page) * 100

# 7: dit soort characters: “//*” “<!” “ =” “//..//”. DONE
def check_special(page):
    special = ["//*", "<!", " =", "//..//", "<!--"]
    count = 0
    for character in special:
        count += page.count(character) * len(character)
    return count / len(page) * 100

# 8: Het aantal srcs DONE
def check_srcs(page):
    return page.count("src") * 3 / len(page) * 100

# 9: Het aantal hrefs DONE
def check_hrefs(page):
    return page.count("href") * 4 / len(page) * 100

# 10: hoe oud de website is DONE
def get_oldness_days(url):
    try:
        domain = urlparse(url).netloc
        loaded_domain = whois.whois(domain) # get the whois data
        date = loaded_domain.get("creation_date")
        if isinstance(date, list): #if there are multiple dates
            return (datetime.datetime.now() - date[0]).total_seconds() / 86400 # return the date difference in days
        else:
            return (datetime.datetime.now() - date).total_seconds() / 86400 # return the date difference in days
    except:
        return 0
#dit zijn handige functies om de rest mee te doen
def get_formattedwords(text):
    words = text.split()  # gets the wordlist
    cleaned_words = []
    for word in words:
        new_word = word.encode('ascii','ignore').decode('utf-8') #makes it ascii

        newer_word = re.sub(r'[^A-Za-z0-9]+|(\w+-\w+)+', '', new_word)
        if newer_word != "":
            cleaned_words.append(newer_word)
    return cleaned_words



# Hier worden de scripts uitgewerk met de gemaakte functies

def get_stuff(q, lock, goodorbad):
    while True:
        # Wij gaan het opslaan in deze volgorde url, goede website, slechte website,(kenmerken) 1, 2, 3, 4, 5, 6, 7, 8, 9, 10
        # de goodorbad moet [1, 0] zijn voor good en [0, 1] zijn voor bad
        url = q.get()
        q.task_done()
        characteristics = [url]
        characteristics.extend(goodorbad)

        # Dit zijn alle benodigdheden, een try statement voor als de website niet meer bestaat.
        try:
            page = requests.get(url, headers={'User-Agent': 'AdsBot-Google'})
            soup = BeautifulSoup(page.text, 'html.parser')
            text = soup.body.get_text(' ', strip=True)
            text_formatted = get_formattedwords(text)
        except:
            continue

        if not len(text_formatted) != 0:
            continue

        #1
        characteristics.append(check_keywords(text.lower(), "triggerwords.txt") / len(text.strip()) * 100)


        # 2
        characteristics.append(spell_check(text_formatted) / len(text_formatted) * 100)

        # 3 en 4
        characteristics.extend(urlcheck(url))

        # 5
        characteristics.append(check_POST(page.text))

        # 6
        characteristics.append(check_preloads(page.text))

        # 7
        characteristics.append(check_special(page.text))

        # 8
        characteristics.append(check_srcs(page.text))

        # 9
        characteristics.append(check_hrefs(page.text))

        # 10
        characteristics.append(get_oldness_days(url))

        with lock:
            print(f"{characteristics},")

if __name__ == '__main__':
    print("doing bad websites \n[")
    bad = open("Badwebsites.txt", "r")
    bad_queue = Queue()
    lock = Lock()

    for i in range(10):
        thread = Thread(target=get_stuff, args=(bad_queue, lock, [0, 1]))
        thread.daemon = True
        thread.start()

    for word in bad.readlines():
        bad_queue.put(word.replace("\n", ""))
    bad_queue.join()

    print("]\ndoing good websites \n[")
    good = open("Goodwebsites.txt", "r")
    good_queue = Queue()
    lock = Lock()

    for i in range(10):
        thread = Thread(target=get_stuff, args=(good_queue, lock, [1, 0]))
        thread.daemon = True
        thread.start()

    for word in good.readlines():
        good_queue.put(word.replace("\n", ""))
    good_queue.join()
    print("]\ndone")





